[2025-06-16T06:54:47.407+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T06:54:47.425+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T06:54:47.440+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T06:54:47.446+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T06:54:47.452+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T06:54:47.487+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:47.488+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:47.489+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T06:54:47.489+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T06:54:47.490+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T06:54:47.497+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T06:54:47.498+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:47.498+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:47.498+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T06:54:47.506+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T06:54:47.506+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T06:54:47.512+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T06:54:47.513+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T06:54:47.533+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T06:54:47.539+0000] {standard_task_runner.py:63} INFO - Started process 199 to run task
[2025-06-16T06:54:47.543+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '134', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmp_01_l32t']
[2025-06-16T06:54:47.544+0000] {standard_task_runner.py:91} INFO - Job 134: Subtask extract.extract_and_load_to_staging
[2025-06-16T06:54:47.545+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7ff936f2d750>]
[2025-06-16T06:54:47.611+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T06:54:47.619+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host a40e01b9e58f
[2025-06-16T06:54:47.620+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 199)
[2025-06-16T06:54:47.621+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 199)
[2025-06-16T06:54:47.622+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T06:54:47.686+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T06:54:47.691+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T06:54:47.717+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T06:54:47.748+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T06:54:47.764+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T06:54:47.766+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T06:54:47.767+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T06:54:47.768+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T06:54:47.806+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T06:54:47.817+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T06:54:52.581+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T06:54:52.593+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T06:54:52.612+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T06:54:57.654+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T06:54:57.668+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T06:54:57.692+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T06:54:59.677+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T06:54:59.680+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/extract_data.py", line 25, in extract_and_load_to_staging
    df = source_operator.get_data_to_pd(f"SELECT * FROM {table}")
  File "/opt/airflow/plugins/mysql_operator.py", line 15, in get_data_to_pd
    return self.mysqlhook.get_pandas_df(query)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/common/sql/hooks/sql.py", line 262, in get_pandas_df
    with closing(self.get_conn()) as conn:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/mysql/hooks/mysql.py", line 196, in get_conn
    return MySQLdb.connect(**conn_config)
  File "/home/airflow/.local/lib/python3.10/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/MySQLdb/connections.py", line 195, in __init__
    super().__init__(*args, **kwargs2)
MySQLdb.OperationalError: (2005, "Unknown server host 'mysql' (-2)")
[2025-06-16T06:54:59.712+0000] {taskinstance.py:562} DEBUG - Task Duration set to 12.21464
[2025-06-16T06:54:59.718+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T06:54:59.719+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T065447, end_date=20250616T065459
[2025-06-16T06:54:59.757+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T06:54:59.759+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 134 for task extract.extract_and_load_to_staging ((2005, "Unknown server host 'mysql' (-2)"); 199)
[2025-06-16T06:54:59.790+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-06-16T06:54:59.823+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 06:54:46.883767+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T06:54:59.824+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 06:54:46.883767+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T06:54:59.825+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.826+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.827+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_shipmode', 'transform.transform_dim_customers', 'transform.transform_dim_date'}
[2025-06-16T06:54:59.828+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_shipmode', 'transform.transform_dim_customers', 'transform.transform_dim_date'}
[2025-06-16T06:54:59.829+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.830+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.831+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T06:54:59.832+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.832+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.833+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.834+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.834+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T06:54:59.836+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.836+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.837+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.838+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.839+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T06:54:59.839+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.840+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.841+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.841+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.842+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T06:54:59.843+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.844+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.844+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T06:54:59.845+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T06:54:59.846+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T06:54:59.847+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.848+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T06:54:59.862+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-16T06:54:59.878+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T09:20:44.774+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T09:20:44.800+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T09:20:44.822+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T09:20:44.832+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T09:20:44.846+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T09:20:44.926+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T09:20:44.926+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:44.927+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T09:20:44.927+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:44.927+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T09:20:44.928+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T09:20:44.935+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:44.940+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T09:20:44.940+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T09:20:44.941+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T09:20:44.942+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:44.942+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T09:20:44.942+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T09:20:44.959+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T09:20:44.968+0000] {standard_task_runner.py:63} INFO - Started process 444 to run task
[2025-06-16T09:20:44.974+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmpo5gff49u']
[2025-06-16T09:20:44.975+0000] {standard_task_runner.py:91} INFO - Job 192: Subtask extract.extract_and_load_to_staging
[2025-06-16T09:20:44.976+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7faa3cb29750>]
[2025-06-16T09:20:45.029+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T09:20:45.036+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host a40e01b9e58f
[2025-06-16T09:20:45.037+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 444)
[2025-06-16T09:20:45.037+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 444)
[2025-06-16T09:20:45.038+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T09:20:45.082+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T09:20:45.085+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T09:20:45.100+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T09:20:45.179+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T09:20:45.323+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T09:20:45.328+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T09:20:45.331+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T09:20:45.334+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T09:20:45.487+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T09:20:45.508+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T09:20:45.723+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T09:20:45.790+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T09:20:45.791+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/extract_data.py", line 27, in extract_and_load_to_staging
    staging_operator.save_data_to_postgres_1(
AttributeError: 'PostgresOperators' object has no attribute 'save_data_to_postgres_1'
[2025-06-16T09:20:45.865+0000] {taskinstance.py:562} DEBUG - Task Duration set to 0.937259
[2025-06-16T09:20:45.914+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T09:20:45.916+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T092044, end_date=20250616T092045
[2025-06-16T09:20:46.006+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T09:20:46.009+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 192 for task extract.extract_and_load_to_staging ('PostgresOperators' object has no attribute 'save_data_to_postgres_1'; 444)
[2025-06-16T09:20:46.064+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-06-16T09:20:46.217+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 09:20:43.828262+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T09:20:46.219+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 09:20:43.828262+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T09:20:46.221+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.222+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.226+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T09:20:46.226+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.227+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.228+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.228+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.229+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T09:20:46.229+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.229+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.230+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.230+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.231+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T09:20:46.232+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.232+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.233+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.233+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.234+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T09:20:46.234+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.234+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.235+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.236+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.236+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T09:20:46.237+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.237+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T09:20:46.238+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T09:20:46.238+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T09:20:46.238+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_locations', 'transform.transform_dim_shipmode', 'transform.transform_dim_products'}
[2025-06-16T09:20:46.239+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_locations', 'transform.transform_dim_shipmode', 'transform.transform_dim_products'}
[2025-06-16T09:20:46.253+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-16T09:20:46.268+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T14:17:14.605+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T14:17:14.640+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T14:17:14.687+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T14:17:14.693+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T14:17:14.712+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T14:17:14.878+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T14:17:14.892+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:14.893+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:14.893+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T14:17:14.894+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T14:17:14.894+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T14:17:14.896+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:14.901+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:14.907+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T14:17:14.907+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T14:17:14.908+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T14:17:14.908+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T14:17:14.909+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T14:17:14.936+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T14:17:14.968+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmpklsuptkm']
[2025-06-16T14:17:14.959+0000] {standard_task_runner.py:63} INFO - Started process 306 to run task
[2025-06-16T14:17:14.971+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract.extract_and_load_to_staging
[2025-06-16T14:17:14.975+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7fee6db2d750>]
[2025-06-16T14:17:15.241+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T14:17:15.253+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host cdd5f2df74dd
[2025-06-16T14:17:15.255+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 306)
[2025-06-16T14:17:15.256+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 306)
[2025-06-16T14:17:15.256+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T14:17:15.295+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T14:17:15.298+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T14:17:15.351+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T14:17:15.377+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T14:17:15.390+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T14:17:15.392+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T14:17:15.392+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T14:17:15.392+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T14:17:15.453+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T14:17:15.460+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T14:17:15.590+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T14:17:15.700+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T14:17:15.701+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "de_psql" (172.18.0.3), port 5432 failed: FATAL:  database "lobalsuperstore" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/extract_data.py", line 27, in extract_and_load_to_staging
    staging_operator.save_data_to_postgres(
  File "/opt/airflow/plugins/postgresql_operator.py", line 20, in save_data_to_postgres
    with self.engine.connect() as conn:
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 145, in _do_get
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "de_psql" (172.18.0.3), port 5432 failed: FATAL:  database "lobalsuperstore" does not exist

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-06-16T14:17:15.834+0000] {taskinstance.py:562} DEBUG - Task Duration set to 0.939563
[2025-06-16T14:17:15.838+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T14:17:15.839+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T141714, end_date=20250616T141715
[2025-06-16T14:17:15.859+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T14:17:15.860+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 3 for task extract.extract_and_load_to_staging ((psycopg2.OperationalError) connection to server at "de_psql" (172.18.0.3), port 5432 failed: FATAL:  database "lobalsuperstore" does not exist

(Background on this error at: https://sqlalche.me/e/14/e3q8); 306)
[2025-06-16T14:17:15.877+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-06-16T14:17:15.900+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 14:17:13.038557+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T14:17:15.901+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 14:17:13.038557+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T14:17:15.902+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.903+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_shipmode', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_products', 'transform.transform_dim_date'}
[2025-06-16T14:17:15.904+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_shipmode', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_products', 'transform.transform_dim_date'}
[2025-06-16T14:17:15.904+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.905+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.905+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T14:17:15.906+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.906+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.907+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.907+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.907+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T14:17:15.908+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.908+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.909+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.909+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.909+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T14:17:15.910+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.910+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.910+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.911+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.911+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T14:17:15.912+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.912+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.913+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.913+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T14:17:15.914+0000] {taskinstance.py:1884} DEBUG - Setting task state for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> to upstream_failed
[2025-06-16T14:17:15.915+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.915+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=1, upstream_failed=0, removed=0, done=1, success_setup=0, skipped_setup=0), upstream_task_ids={'extract.extract_and_load_to_staging'}
[2025-06-16T14:17:15.916+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T14:17:15.921+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-06-16T14:17:15.927+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T15:51:44.791+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T15:51:44.837+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T15:51:44.867+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T15:51:44.879+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T15:51:44.931+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T15:51:45.127+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T15:51:45.127+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T15:51:45.128+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:51:45.129+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:51:45.135+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T15:51:45.135+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T15:51:45.145+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T15:51:45.146+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T15:51:45.146+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:51:45.147+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:51:45.154+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T15:51:45.154+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T15:51:45.155+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T15:51:45.178+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T15:51:45.190+0000] {standard_task_runner.py:63} INFO - Started process 579 to run task
[2025-06-16T15:51:45.195+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '136', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmpo8dcm32q']
[2025-06-16T15:51:45.196+0000] {standard_task_runner.py:91} INFO - Job 136: Subtask extract.extract_and_load_to_staging
[2025-06-16T15:51:45.199+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7fa267925750>]
[2025-06-16T15:51:45.253+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T15:51:45.257+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host 9b7c60adfed6
[2025-06-16T15:51:45.257+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 579)
[2025-06-16T15:51:45.258+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 579)
[2025-06-16T15:51:45.258+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T15:51:45.296+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T15:51:45.298+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T15:51:45.339+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T15:51:45.367+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T15:51:45.383+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T15:51:45.384+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T15:51:45.384+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T15:51:45.385+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T15:51:49.793+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T15:51:49.794+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Category...
[2025-06-16T15:51:49.800+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:51:50.071+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:51:50.240+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T15:51:50.355+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T15:51:50.436+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T15:51:50.661+0000] {logging_mixin.py:188} INFO -   load Category  staging.stg_Category
[2025-06-16T15:51:50.662+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Customer...
[2025-06-16T15:51:50.695+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:51:50.708+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:51:51.094+0000] {logging_mixin.py:188} INFO -   load Customer  staging.stg_Customer
[2025-06-16T15:51:51.094+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Order_Details...
[2025-06-16T15:51:51.100+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:51:51.107+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:51:55.469+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T15:51:55.496+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T15:51:55.508+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T15:52:00.543+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T15:52:00.553+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T15:52:00.601+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T15:52:05.080+0000] {logging_mixin.py:188} INFO -   load Order_Details  staging.stg_Order_Details
[2025-06-16T15:52:05.080+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Orders...
[2025-06-16T15:52:05.089+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:52:05.096+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:52:05.635+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T15:52:05.648+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T15:52:05.661+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T15:52:09.789+0000] {logging_mixin.py:188} INFO -   load Orders  staging.stg_Orders
[2025-06-16T15:52:09.789+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Product...
[2025-06-16T15:52:09.799+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:52:09.808+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:52:10.606+0000] {logging_mixin.py:188} INFO -   load Product  staging.stg_Product
[2025-06-16T15:52:10.606+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Sub_Category...
[2025-06-16T15:52:10.614+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T15:52:10.623+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T15:52:10.677+0000] {logging_mixin.py:188} INFO -   load Sub_Category  staging.stg_Sub_Category
[2025-06-16T15:52:10.678+0000] {python.py:237} INFO - Done. Returned value was: None
[2025-06-16T15:52:10.680+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T15:52:10.681+0000] {__init__.py:77} DEBUG - Lineage called with inlets: [], outlets: []
[2025-06-16T15:52:10.694+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T15:52:10.695+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T15:52:10.696+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T155145, end_date=20250616T155210
[2025-06-16T15:52:10.697+0000] {taskinstance.py:562} DEBUG - Task Duration set to 25.560363
[2025-06-16T15:52:10.704+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T15:52:10.722+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T15:52:10.728+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T15:52:10.817+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-16T15:52:10.841+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 15:51:42.955143+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T15:52:10.843+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 15:51:42.955143+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T15:52:10.844+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.846+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.847+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T15:52:10.848+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.849+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.849+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T15:52:10.849+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.850+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.850+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T15:52:10.850+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.851+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.851+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T15:52:10.852+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.852+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.852+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T15:52:10.853+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T15:52:10.853+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_shipmode'}
[2025-06-16T15:52:10.853+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_shipmode'}
[2025-06-16T15:52:10.854+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T15:52:10.868+0000] {taskinstance.py:3503} INFO - 5 downstream tasks scheduled from follow-on schedule check
[2025-06-16T15:52:10.871+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T16:04:05.961+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:04:05.983+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T16:04:05.995+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:04:05.998+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T16:04:06.006+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T16:04:06.051+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T16:04:06.052+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T16:04:06.053+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:06.054+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:06.062+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:04:06.063+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:04:06.069+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T16:04:06.070+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T16:04:06.071+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:06.071+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:06.075+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:04:06.076+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:04:06.076+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T16:04:06.173+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T16:04:06.181+0000] {standard_task_runner.py:63} INFO - Started process 1009 to run task
[2025-06-16T16:04:06.197+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '157', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmplzm_83na']
[2025-06-16T16:04:06.199+0000] {standard_task_runner.py:91} INFO - Job 157: Subtask extract.extract_and_load_to_staging
[2025-06-16T16:04:06.202+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7fa267925750>]
[2025-06-16T16:04:06.354+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:04:06.359+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host 9b7c60adfed6
[2025-06-16T16:04:06.359+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 1009)
[2025-06-16T16:04:06.360+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 1009)
[2025-06-16T16:04:06.360+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T16:04:06.455+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:04:06.457+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T16:04:06.472+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T16:04:06.627+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T16:04:06.639+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T16:04:06.640+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T16:04:06.641+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T16:04:06.641+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T16:04:07.334+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T16:04:07.337+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Category...
[2025-06-16T16:04:07.356+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:07.402+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:07.544+0000] {logging_mixin.py:188} INFO -   load Category  staging.stg_Category
[2025-06-16T16:04:07.546+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Customer...
[2025-06-16T16:04:07.557+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:07.568+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:07.942+0000] {logging_mixin.py:188} INFO -   load Customer  staging.stg_Customer
[2025-06-16T16:04:07.944+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Order_Details...
[2025-06-16T16:04:07.962+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:07.985+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:11.216+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:04:11.224+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:04:11.242+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:04:16.273+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:04:16.286+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:04:16.304+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:04:21.190+0000] {logging_mixin.py:188} INFO -   load Order_Details  staging.stg_Order_Details
[2025-06-16T16:04:21.192+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Orders...
[2025-06-16T16:04:21.227+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:21.250+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:21.310+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:04:21.323+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:04:21.368+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:04:26.398+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:04:26.410+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:04:26.433+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:04:30.917+0000] {logging_mixin.py:188} INFO -   load Orders  staging.stg_Orders
[2025-06-16T16:04:30.919+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Product...
[2025-06-16T16:04:30.937+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:30.966+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:31.473+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:04:31.487+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:04:31.511+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:04:32.476+0000] {logging_mixin.py:188} INFO -   load Product  staging.stg_Product
[2025-06-16T16:04:32.477+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Sub_Category...
[2025-06-16T16:04:32.490+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:04:32.501+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:04:32.542+0000] {logging_mixin.py:188} INFO -   load Sub_Category  staging.stg_Sub_Category
[2025-06-16T16:04:32.542+0000] {python.py:237} INFO - Done. Returned value was: None
[2025-06-16T16:04:32.543+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T16:04:32.543+0000] {__init__.py:77} DEBUG - Lineage called with inlets: [], outlets: []
[2025-06-16T16:04:32.554+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T16:04:32.554+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T160406, end_date=20250616T160432
[2025-06-16T16:04:32.555+0000] {taskinstance.py:562} DEBUG - Task Duration set to 26.490396
[2025-06-16T16:04:32.568+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T16:04:32.616+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-16T16:04:32.644+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:04:04.918626+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T16:04:32.645+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:04:04.918626+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T16:04:32.646+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.648+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.649+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:04:32.650+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.651+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.651+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:04:32.651+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.652+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.652+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:04:32.652+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.653+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.653+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:04:32.653+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.654+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.654+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:04:32.655+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:04:32.655+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_shipmode'}
[2025-06-16T16:04:32.656+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_shipmode'}
[2025-06-16T16:04:32.656+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:04:32.670+0000] {taskinstance.py:3503} INFO - 5 downstream tasks scheduled from follow-on schedule check
[2025-06-16T16:04:32.673+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T16:36:54.258+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:36:54.302+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T16:36:54.337+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:36:54.354+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T16:36:54.371+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T16:36:54.450+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T16:36:54.451+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:36:54.452+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:36:54.462+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:36:54.465+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T16:36:54.466+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:36:54.467+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:36:54.474+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:36:54.482+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T16:36:54.483+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T16:36:54.484+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:36:54.484+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:36:54.485+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T16:36:54.510+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T16:36:54.532+0000] {standard_task_runner.py:63} INFO - Started process 200 to run task
[2025-06-16T16:36:54.539+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '197', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmpezj9r1my']
[2025-06-16T16:36:54.541+0000] {standard_task_runner.py:91} INFO - Job 197: Subtask extract.extract_and_load_to_staging
[2025-06-16T16:36:54.543+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7fc637a79750>]
[2025-06-16T16:36:54.610+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:36:54.616+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host 9b7c60adfed6
[2025-06-16T16:36:54.617+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 200)
[2025-06-16T16:36:54.618+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 200)
[2025-06-16T16:36:54.618+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T16:36:54.669+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:36:54.672+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T16:36:54.697+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T16:36:54.760+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T16:36:54.786+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T16:36:54.787+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T16:36:54.788+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T16:36:54.789+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T16:36:54.827+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T16:36:54.830+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Category...
[2025-06-16T16:36:54.839+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:36:55.045+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:36:55.621+0000] {logging_mixin.py:188} INFO -   load Category  staging.stg_Category
[2025-06-16T16:36:55.623+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Customer...
[2025-06-16T16:36:55.650+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:36:55.664+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:36:56.549+0000] {logging_mixin.py:188} INFO -   load Customer  staging.stg_Customer
[2025-06-16T16:36:56.550+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Order_Details...
[2025-06-16T16:36:56.561+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:36:56.567+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:36:59.575+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:36:59.593+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:36:59.619+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:37:04.621+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:37:04.655+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:37:04.703+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:37:09.706+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:37:09.747+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:37:09.796+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:37:14.787+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:37:14.794+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:37:15.408+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:37:16.841+0000] {logging_mixin.py:188} INFO -   load Order_Details  staging.stg_Order_Details
[2025-06-16T16:37:16.845+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Orders...
[2025-06-16T16:37:16.867+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:37:16.898+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:37:20.446+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:37:20.457+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:37:20.995+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:37:22.531+0000] {logging_mixin.py:188} INFO -   load Orders  staging.stg_Orders
[2025-06-16T16:37:22.532+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Product...
[2025-06-16T16:37:22.546+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:37:22.553+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:37:23.509+0000] {logging_mixin.py:188} INFO -   load Product  staging.stg_Product
[2025-06-16T16:37:23.510+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Sub_Category...
[2025-06-16T16:37:23.526+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:37:23.534+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:37:23.597+0000] {logging_mixin.py:188} INFO -   load Sub_Category  staging.stg_Sub_Category
[2025-06-16T16:37:23.598+0000] {python.py:237} INFO - Done. Returned value was: None
[2025-06-16T16:37:23.599+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T16:37:23.600+0000] {__init__.py:77} DEBUG - Lineage called with inlets: [], outlets: []
[2025-06-16T16:37:23.617+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T16:37:23.619+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T163654, end_date=20250616T163723
[2025-06-16T16:37:23.619+0000] {taskinstance.py:562} DEBUG - Task Duration set to 29.150951
[2025-06-16T16:37:23.645+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T16:37:23.706+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-16T16:37:23.758+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:36:53.011823+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T16:37:23.760+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:36:53.011823+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T16:37:23.761+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.762+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.764+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:37:23.765+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.766+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.766+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:37:23.767+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.767+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.768+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:37:23.769+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.769+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.770+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:37:23.771+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.771+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.772+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:37:23.772+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:37:23.773+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:37:23.773+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_products', 'transform.transform_dim_shipmode'}
[2025-06-16T16:37:23.774+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_locations', 'transform.transform_dim_customers', 'transform.transform_dim_date', 'transform.transform_dim_products', 'transform.transform_dim_shipmode'}
[2025-06-16T16:37:23.794+0000] {taskinstance.py:3503} INFO - 5 downstream tasks scheduled from follow-on schedule check
[2025-06-16T16:37:23.797+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2025-06-16T16:52:20.231+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:52:20.257+0000] {__init__.py:54} DEBUG - Loading core task runner: StandardTaskRunner
[2025-06-16T16:52:20.277+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:52:20.281+0000] {base_task_runner.py:73} DEBUG - Planning to run as the  user
[2025-06-16T16:52:20.311+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-06-16T16:52:20.531+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2025-06-16T16:52:20.532+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:52:20.534+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2025-06-16T16:52:20.548+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:20.550+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:20.552+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:52:20.559+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2025-06-16T16:52:20.562+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:20.563+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:20.571+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2025-06-16T16:52:20.572+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2025-06-16T16:52:20.572+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [queued]>
[2025-06-16T16:52:20.573+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-06-16T16:52:20.604+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): extract.extract_and_load_to_staging> on 2025-06-15 00:00:00+00:00
[2025-06-16T16:52:20.629+0000] {standard_task_runner.py:63} INFO - Started process 416 to run task
[2025-06-16T16:52:20.633+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'globalsuperstore', 'extract.extract_and_load_to_staging', 'scheduled__2025-06-15T00:00:00+00:00', '--job-id', '231', '--raw', '--subdir', 'DAGS_FOLDER/global_superstore_dw_etl.py', '--cfg-path', '/tmp/tmp8zf9o55y']
[2025-06-16T16:52:20.634+0000] {standard_task_runner.py:91} INFO - Job 231: Subtask extract.extract_and_load_to_staging
[2025-06-16T16:52:20.636+0000] {cli_action_loggers.py:70} DEBUG - Calling callbacks: [<function default_action_log at 0x7f3de5b29750>]
[2025-06-16T16:52:20.694+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:52:20.698+0000] {task_command.py:426} INFO - Running <TaskInstance: globalsuperstore.extract.extract_and_load_to_staging scheduled__2025-06-15T00:00:00+00:00 [running]> on host 9b7c60adfed6
[2025-06-16T16:52:20.698+0000] {settings.py:363} DEBUG - Disposing DB connection pool (PID 416)
[2025-06-16T16:52:20.699+0000] {settings.py:250} DEBUG - Setting up DB connection pool (PID 416)
[2025-06-16T16:52:20.699+0000] {settings.py:298} DEBUG - settings.prepare_engine_args(): Using NullPool
[2025-06-16T16:52:20.736+0000] {taskinstance.py:1042} DEBUG - previous_execution_date was called
[2025-06-16T16:52:20.739+0000] {taskinstance.py:1845} DEBUG - Clearing XCom data
[2025-06-16T16:52:20.788+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields.write with retries. Try 1 of 3
[2025-06-16T16:52:20.835+0000] {retries.py:91} DEBUG - Running RenderedTaskInstanceFields._do_delete_old_records with retries. Try 1 of 3
[2025-06-16T16:52:20.869+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='globalsuperstore' AIRFLOW_CTX_TASK_ID='extract.extract_and_load_to_staging' AIRFLOW_CTX_EXECUTION_DATE='2025-06-15T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-06-15T00:00:00+00:00'
[2025-06-16T16:52:20.871+0000] {__init__.py:115} DEBUG - Preparing lineage inlets and outlets
[2025-06-16T16:52:20.872+0000] {__init__.py:154} DEBUG - inlets: [], outlets: []
[2025-06-16T16:52:20.873+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-06-16T16:52:20.975+0000] {base.py:84} INFO - Using connection ID 'postgres' for task execution.
[2025-06-16T16:52:20.979+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Category...
[2025-06-16T16:52:20.991+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:21.349+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:21.978+0000] {logging_mixin.py:188} INFO -   load Category  staging.stg_Category
[2025-06-16T16:52:21.978+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Customer...
[2025-06-16T16:52:21.988+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:21.998+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:25.674+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:25.714+0000] {logging_mixin.py:188} INFO -   load Customer  staging.stg_Customer
[2025-06-16T16:52:25.715+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Order_Details...
[2025-06-16T16:52:25.736+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:25.766+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:25.768+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:25.773+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:30.809+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:30.823+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:30.844+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:35.868+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:35.950+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:36.016+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:44.234+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:44.334+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:44.393+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:48.424+0000] {logging_mixin.py:188} INFO -   load Order_Details  staging.stg_Order_Details
[2025-06-16T16:52:48.425+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Orders...
[2025-06-16T16:52:48.437+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:48.491+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:49.428+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:49.443+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:49.466+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:54.470+0000] {retries.py:91} DEBUG - Running Job._fetch_from_db with retries. Try 1 of 3
[2025-06-16T16:52:54.480+0000] {retries.py:91} DEBUG - Running Job._update_heartbeat with retries. Try 1 of 3
[2025-06-16T16:52:54.500+0000] {job.py:214} DEBUG - [heartbeat]
[2025-06-16T16:52:55.428+0000] {logging_mixin.py:188} INFO -   load Orders  staging.stg_Orders
[2025-06-16T16:52:55.429+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Product...
[2025-06-16T16:52:55.443+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:55.460+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:57.392+0000] {logging_mixin.py:188} INFO -   load Product  staging.stg_Product
[2025-06-16T16:52:57.393+0000] {logging_mixin.py:188} INFO -  ang ti d liu t bng Sub_Category...
[2025-06-16T16:52:57.405+0000] {base.py:84} INFO - Using connection ID 'mysql' for task execution.
[2025-06-16T16:52:57.422+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.10/site-packages/***/providers/common/sql/hooks/sql.py:263 UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
[2025-06-16T16:52:57.505+0000] {logging_mixin.py:188} INFO -   load Sub_Category  staging.stg_Sub_Category
[2025-06-16T16:52:57.507+0000] {python.py:237} INFO - Done. Returned value was: None
[2025-06-16T16:52:57.508+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-06-16T16:52:57.509+0000] {__init__.py:77} DEBUG - Lineage called with inlets: [], outlets: []
[2025-06-16T16:52:57.525+0000] {taskinstance.py:584} DEBUG - Clearing next_method and next_kwargs.
[2025-06-16T16:52:57.526+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=globalsuperstore, task_id=extract.extract_and_load_to_staging, run_id=scheduled__2025-06-15T00:00:00+00:00, execution_date=20250615T000000, start_date=20250616T165220, end_date=20250616T165257
[2025-06-16T16:52:57.526+0000] {taskinstance.py:562} DEBUG - Task Duration set to 36.972516
[2025-06-16T16:52:57.551+0000] {cli_action_loggers.py:88} DEBUG - Calling callbacks: []
[2025-06-16T16:52:57.610+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-06-16T16:52:57.668+0000] {dagrun.py:931} DEBUG - number of tis tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:52:17.905108+00:00. externally triggered: False>: 7 task(s)
[2025-06-16T16:52:57.673+0000] {dagrun.py:952} DEBUG - number of scheduleable tasks for <DagRun globalsuperstore @ 2025-06-15 00:00:00+00:00: scheduled__2025-06-15T00:00:00+00:00, state:running, queued_at: 2025-06-16 16:52:17.905108+00:00. externally triggered: False>: 6 task(s)
[2025-06-16T16:52:57.687+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.688+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.688+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_customers scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:52:57.690+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.691+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.691+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_products scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:52:57.691+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.692+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.692+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_locations scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:52:57.693+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.693+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.693+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_shipmode scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:52:57.694+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.694+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.694+0000] {taskinstance.py:2076} DEBUG - Dependencies all met for dep_context=None ti=<TaskInstance: globalsuperstore.transform.transform_dim_date scheduled__2025-06-15T00:00:00+00:00 [None]>
[2025-06-16T16:52:57.695+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_date', 'transform.transform_dim_shipmode', 'transform.transform_dim_locations', 'transform.transform_dim_customers'}
[2025-06-16T16:52:57.695+0000] {taskinstance.py:2066} DEBUG - Dependencies not met for <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 5 non-success(es). upstream_states=_UpstreamTIStates(success=0, skipped=0, failed=0, upstream_failed=0, removed=0, done=0, success_setup=0, skipped_setup=0), upstream_task_ids={'transform.transform_dim_products', 'transform.transform_dim_date', 'transform.transform_dim_shipmode', 'transform.transform_dim_locations', 'transform.transform_dim_customers'}
[2025-06-16T16:52:57.698+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2025-06-16T16:52:57.699+0000] {taskinstance.py:2088} DEBUG - <TaskInstance: globalsuperstore.load.transform_fact_sales scheduled__2025-06-15T00:00:00+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2025-06-16T16:52:57.726+0000] {taskinstance.py:3503} INFO - 5 downstream tasks scheduled from follow-on schedule check
[2025-06-16T16:52:57.732+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
